import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta 
import sys
from pyspark.sql import SparkSession
import pyspark.sql.functions as func
from pyspark.sql.types import *
import logging
import lightgbm as lgb

os.environ["PYSPARK_PYTHON"] = "/usr/bin/python3.7"
QueueName = os.environ["yarnQueueName"] 
spark = SparkSession.builder.\
            config('spark.executor.memory', '14g').\
            config('spark.executor.cores', '4').\
            config('spark.driver.memory','12g').\
            config('spark.executor.instances', '80').\
            config('spark.driver.maxResultSize', '10g').\
            config('spark.sql.autoBroadcastJoinThreshold',-1).\
            config("spark.yarn.queue", QueueName).\
            config("hive.exec.orc.split.strategy", "ETL").\
            config("spark.sql.hive.convertMetastoreOrc", True).\
            config("spark.sql.orc.impl", "native").\
            config("spark.driver.allowMultipleContexts", False).\
            config("spark.debug.maxToStringFields", "100").\
            config("spark.sparkContext.setLogLevel", "ERROR").\
            config("hive.exec.dynamic.partition.mode", "nonstrict").\
            config('spark.shuffle.io.connectionTimeout','60s').\
            config('spark.shuffle.io.maxRetries','10').\
            config('spark.shuffle.io.retryWait','15s').\
            config('spark.sql.join.preferSortMergeJoin','true').\
            config('spark.sql.bucketing.enabled','true').\
            config('spark.sql.sortMergeJoinBufferSize','256000000').\
            appName('prea_v01001').\
            enableHiveSupport().getOrCreate()

spark.sparkContext.setLogLevel('Error')

from pyspark.sql.functions import col, when, lit, log, expr
from pyspark.sql.types import NumericType, DoubleType, IntegerType
from pyspark.sql.functions import pandas_udf,udf
import joblib
from tqdm import tqdm

pt = str(sys.argv[1])
print(pt)

hive_fes_map = {
    ‘’:[]
}

hive_list1 = list(hive_fes_map.ke ys())

cols_list_all = []
for hive_one in hive_list1:
    cols_list_all = cols_list_all+hive_fes_map[hive_one]

ft_df = spark.sql(f'''
select user_guid
from fintech.dws_plat_user_label_info_df
where pt='{pt}'
and user_guid<>''
and user_guid is not null
group by user_guid

''')

for t in tqdm(hive_list1):
    ft_list = hive_fes_map[t]
    ft_df = spark.table(t).filter(f" pt = {pt}").select( ['user_guid'] + ft_list ).join(ft_df,on = ['user_guid'], how = 'right')
ft_df = ft_df.dropDuplicates(subset=['user_guid'])
print('hive表拼接完成！！！')

ft_df = ft_df.withColumn('pudao_zijie_external_90002024_rank_v1', when(col('pudao_zijie_external_90002024_rank_v1')=='A', '1')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='B', '2')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='C', '3')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='D', '4')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='E', '5')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='F', '6')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='G', '7')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='H', '8')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='I', '9')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='J', '10')
                        .when(col('pudao_zijie_external_90002024_rank_v1')=='K', '11')                      
                        .otherwise(None))


for fes_one in cols_list_all:
    ft_df = ft_df.withColumn(fes_one, ft_df[fes_one].cast(DoubleType()))

print('数据格式处理完成！！！')

ft_df.registerTempTable('temp_table1')

spark.sql(f"""
insert overwrite table fintech_risk.fpt_pa_rta_prea_v01001_feas_df partition(pt ='{pt}')

select 
    user_guid
    ,min_ridetime_all_ride_bike_720d
    

from temp_table1
""")

print('入参写入完成！！!')

ft_df = spark.sql(f'''
select *
from fintech_risk.fpt_pa_rta_prea_v01001_feas_df
where pt='{pt}'

''')


rename_map = {
    'pudao_zijie_external_90001024_rank_v1':'pudao_zijie_external_90001024_rank_v1_cd',
    'pudao_zijie_external_90002024_rank_v1':'pudao_zijie_external_90002024_rank_v1_cd'
    
}

columns_all = ft_df.columns

select_expr = [
    ft_df[col].alias(rename_map[col]) if col in rename_map else ft_df[col] 
    for col in columns_all
]

ft_df = ft_df.select(*select_expr)

print('特征重命名因完成！！！')

cols_list_all_new = list(set(ft_df.columns)-set(['user_guid', 'pt']))
special_list = [-999]
for fes_one in cols_list_all_new:
    ft_df = ft_df.withColumn(fes_one, when(col(fes_one).isin(special_list), None)
                                .otherwise(col(fes_one)))
    
filter_null = (col(cols_list_all_new[0]).isNull())
for fes_one in cols_list_all_new[1:]:
    filter_null = filter_null&(col(fes_one).isNull())

ft_df = ft_df.withColumn(
    "flag_null_1",
    when(filter_null, 1)
    .otherwise(0))

print('数据预处理完成！！！')


model_flod1 = lgb.Booster(model_file='/home/master/prod/scriptfile/pa_rta_preA_v1_lgb_flod_1.txt')


model_flod1_fes = model_flod1.feature_name()


print('模型文件读取完成！！！')

sc = spark.sparkContext
model_flod1_fes = sc.broadcast(model_flod1_fes)


def predict_model_flod1(feature_list,):
    @pandas_udf(returnType=DoubleType())
    def predict_score(*features):
        X = pd.concat(features, axis=1).values
        y = model_flod1.predict(X)
        return pd.Series(y)
    return predict_score




ft_df = ft_df.withColumn("pa_rta_prea_score_flod_1", predict_model_flod1(model_flod1_fes.value)(
            *[ft_df[f] for f in model_flod1_fes.value]))



score_df = ft_df.select('user_guid', 'pa_rta_prea_score_flod_1', 'pa_rta_prea_score_flod_2', 'pa_rta_prea_score_flod_3', 'pa_rta_prea_score_flod_4', 'pa_rta_prea_score_flod_5', 'flag_null_1')

score_df = score_df.withColumn(
    "mdl_loan_risk_pa_rta_prea_score_v01001_offline", 
    expr("(pa_rta_prea_score_flod_1 + pa_rta_prea_score_flod_2 + pa_rta_prea_score_flod_3 + pa_rta_prea_score_flod_4 + pa_rta_prea_score_flod_5) / 5")
)

score_df = score_df.withColumn('mdl_loan_risk_pa_rta_prea_score_v01001_offline', when(col('flag_null_1')==1, -1)
                                .otherwise(col('mdl_loan_risk_pa_rta_prea_score_v01001_offline')))


score_df.registerTempTable('temp_table2')

spark.sql(f"""
insert overwrite table fintech_risk.fpt_pa_rta_prea_v01001_score_df partition(pt ='{pt}')

select 
    user_guid
    ,mdl_loan_risk_pa_rta_prea_score_v01001_offline
    ,flag_null_1
from temp_table2

""")


print("模型分写入完成！！！")
